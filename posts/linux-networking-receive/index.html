<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>网络数据包接受过程分析——从网卡到内核协议栈（以Intel e1000 + Linux 4.4为例） | rand0m's blog</title><meta name=keywords content="Networking,Linux,C"><meta name=description content="引言
网络数据包从网卡到应用程序，需要经历一段复杂的旅程。作为开发者，我们平时调用 socket()、recv() 就能轻松拿到数据，却很少思考内核背后究竟发生了什么。
本系列文章尝试结合 理论流程 + 内核源码分析，逐步剖析 Linux 内核中网络数据包的接收过程。这里选择 Linux 4.4 内核作为例子（代码相对稳定，资料丰富，逻辑上没有过多新特性干扰），并结合 Intel e1000 驱动来具体展示数据包是如何从网卡到达内核网络协议栈的。
网络数据包接收的总体流程
先给出一个全局视角：数据包从网卡到达内存，再到协议栈的路径，大致如下：

数据包到达网卡
网卡硬件接收以太帧，做基本校验（如 CRC）。
DMA 写入内存
网卡通过 DMA 将数据包写入驱动预先分配好的接收缓冲区（Descriptor Ring）。
中断通知 CPU
网卡通过 IRQ 告诉 CPU：“我收到了新数据包”。
驱动中断处理函数（ISR）
驱动快速处理中断，通常只是调用 __napi_schedule()，把 NAPI poll 加入调度队列。
软中断调度 NAPI poll
CPU 执行 do_softirq() → net_rx_action() → 调用 e1000 的 poll 函数。
poll 函数提取数据包并构造 skb
驱动在 poll 中读取 DMA ring 的描述符，把数据包封装进 sk_buff 结构，交给内核网络子系统。
协议栈处理
skb 被送到 IP 层，进一步交给 TCP/UDP，最终到达 socket，供应用程序读取。

"><meta name=author content="rand0m"><link rel=canonical href=https://rand0m42195.github.io/posts/linux-networking-receive/><link crossorigin=anonymous href=/assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as=style><link rel=icon href=https://rand0m42195.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://rand0m42195.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://rand0m42195.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://rand0m42195.github.io/apple-touch-icon.png><link rel=mask-icon href=https://rand0m42195.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://rand0m42195.github.io/posts/linux-networking-receive/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://rand0m42195.github.io/posts/linux-networking-receive/"><meta property="og:site_name" content="rand0m's blog"><meta property="og:title" content="网络数据包接受过程分析——从网卡到内核协议栈（以Intel e1000 + Linux 4.4为例）"><meta property="og:description" content="引言 网络数据包从网卡到应用程序，需要经历一段复杂的旅程。作为开发者，我们平时调用 socket()、recv() 就能轻松拿到数据，却很少思考内核背后究竟发生了什么。
本系列文章尝试结合 理论流程 + 内核源码分析，逐步剖析 Linux 内核中网络数据包的接收过程。这里选择 Linux 4.4 内核作为例子（代码相对稳定，资料丰富，逻辑上没有过多新特性干扰），并结合 Intel e1000 驱动来具体展示数据包是如何从网卡到达内核网络协议栈的。
网络数据包接收的总体流程 先给出一个全局视角：数据包从网卡到达内存，再到协议栈的路径，大致如下：
数据包到达网卡 网卡硬件接收以太帧，做基本校验（如 CRC）。 DMA 写入内存 网卡通过 DMA 将数据包写入驱动预先分配好的接收缓冲区（Descriptor Ring）。 中断通知 CPU 网卡通过 IRQ 告诉 CPU：“我收到了新数据包”。 驱动中断处理函数（ISR） 驱动快速处理中断，通常只是调用 __napi_schedule()，把 NAPI poll 加入调度队列。 软中断调度 NAPI poll CPU 执行 do_softirq() → net_rx_action() → 调用 e1000 的 poll 函数。 poll 函数提取数据包并构造 skb 驱动在 poll 中读取 DMA ring 的描述符，把数据包封装进 sk_buff 结构，交给内核网络子系统。 协议栈处理 skb 被送到 IP 层，进一步交给 TCP/UDP，最终到达 socket，供应用程序读取。 "><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-22T20:43:51+08:00"><meta property="article:modified_time" content="2025-09-22T20:43:51+08:00"><meta property="article:tag" content="Networking"><meta property="article:tag" content="Linux"><meta property="article:tag" content="C"><meta name=twitter:card content="summary"><meta name=twitter:title content="网络数据包接受过程分析——从网卡到内核协议栈（以Intel e1000 + Linux 4.4为例）"><meta name=twitter:description content="引言
网络数据包从网卡到应用程序，需要经历一段复杂的旅程。作为开发者，我们平时调用 socket()、recv() 就能轻松拿到数据，却很少思考内核背后究竟发生了什么。
本系列文章尝试结合 理论流程 + 内核源码分析，逐步剖析 Linux 内核中网络数据包的接收过程。这里选择 Linux 4.4 内核作为例子（代码相对稳定，资料丰富，逻辑上没有过多新特性干扰），并结合 Intel e1000 驱动来具体展示数据包是如何从网卡到达内核网络协议栈的。
网络数据包接收的总体流程
先给出一个全局视角：数据包从网卡到达内存，再到协议栈的路径，大致如下：

数据包到达网卡
网卡硬件接收以太帧，做基本校验（如 CRC）。
DMA 写入内存
网卡通过 DMA 将数据包写入驱动预先分配好的接收缓冲区（Descriptor Ring）。
中断通知 CPU
网卡通过 IRQ 告诉 CPU：“我收到了新数据包”。
驱动中断处理函数（ISR）
驱动快速处理中断，通常只是调用 __napi_schedule()，把 NAPI poll 加入调度队列。
软中断调度 NAPI poll
CPU 执行 do_softirq() → net_rx_action() → 调用 e1000 的 poll 函数。
poll 函数提取数据包并构造 skb
驱动在 poll 中读取 DMA ring 的描述符，把数据包封装进 sk_buff 结构，交给内核网络子系统。
协议栈处理
skb 被送到 IP 层，进一步交给 TCP/UDP，最终到达 socket，供应用程序读取。

"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://rand0m42195.github.io/posts/"},{"@type":"ListItem","position":2,"name":"网络数据包接受过程分析——从网卡到内核协议栈（以Intel e1000 + Linux 4.4为例）","item":"https://rand0m42195.github.io/posts/linux-networking-receive/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"网络数据包接受过程分析——从网卡到内核协议栈（以Intel e1000 + Linux 4.4为例）","name":"网络数据包接受过程分析——从网卡到内核协议栈（以Intel e1000 \u002b Linux 4.4为例）","description":"引言 网络数据包从网卡到应用程序，需要经历一段复杂的旅程。作为开发者，我们平时调用 socket()、recv() 就能轻松拿到数据，却很少思考内核背后究竟发生了什么。\n本系列文章尝试结合 理论流程 + 内核源码分析，逐步剖析 Linux 内核中网络数据包的接收过程。这里选择 Linux 4.4 内核作为例子（代码相对稳定，资料丰富，逻辑上没有过多新特性干扰），并结合 Intel e1000 驱动来具体展示数据包是如何从网卡到达内核网络协议栈的。\n网络数据包接收的总体流程 先给出一个全局视角：数据包从网卡到达内存，再到协议栈的路径，大致如下：\n数据包到达网卡 网卡硬件接收以太帧，做基本校验（如 CRC）。 DMA 写入内存 网卡通过 DMA 将数据包写入驱动预先分配好的接收缓冲区（Descriptor Ring）。 中断通知 CPU 网卡通过 IRQ 告诉 CPU：“我收到了新数据包”。 驱动中断处理函数（ISR） 驱动快速处理中断，通常只是调用 __napi_schedule()，把 NAPI poll 加入调度队列。 软中断调度 NAPI poll CPU 执行 do_softirq() → net_rx_action() → 调用 e1000 的 poll 函数。 poll 函数提取数据包并构造 skb 驱动在 poll 中读取 DMA ring 的描述符，把数据包封装进 sk_buff 结构，交给内核网络子系统。 协议栈处理 skb 被送到 IP 层，进一步交给 TCP/UDP，最终到达 socket，供应用程序读取。 ","keywords":["Networking","Linux","C"],"articleBody":"引言 网络数据包从网卡到应用程序，需要经历一段复杂的旅程。作为开发者，我们平时调用 socket()、recv() 就能轻松拿到数据，却很少思考内核背后究竟发生了什么。\n本系列文章尝试结合 理论流程 + 内核源码分析，逐步剖析 Linux 内核中网络数据包的接收过程。这里选择 Linux 4.4 内核作为例子（代码相对稳定，资料丰富，逻辑上没有过多新特性干扰），并结合 Intel e1000 驱动来具体展示数据包是如何从网卡到达内核网络协议栈的。\n网络数据包接收的总体流程 先给出一个全局视角：数据包从网卡到达内存，再到协议栈的路径，大致如下：\n数据包到达网卡 网卡硬件接收以太帧，做基本校验（如 CRC）。 DMA 写入内存 网卡通过 DMA 将数据包写入驱动预先分配好的接收缓冲区（Descriptor Ring）。 中断通知 CPU 网卡通过 IRQ 告诉 CPU：“我收到了新数据包”。 驱动中断处理函数（ISR） 驱动快速处理中断，通常只是调用 __napi_schedule()，把 NAPI poll 加入调度队列。 软中断调度 NAPI poll CPU 执行 do_softirq() → net_rx_action() → 调用 e1000 的 poll 函数。 poll 函数提取数据包并构造 skb 驱动在 poll 中读取 DMA ring 的描述符，把数据包封装进 sk_buff 结构，交给内核网络子系统。 协议栈处理 skb 被送到 IP 层，进一步交给 TCP/UDP，最终到达 socket，供应用程序读取。 在开始具体分析之前，先说以下相关源码的位置。和网卡相关的代码在驱动目录下（/drivers/net/ethernet），由于我们分析的是Intel的e1000网卡，所以具体位置就是/drivers/net/ethernet/intel/e1000，内核网络协议栈位于网络子系统目录下/net，主要是/net/core/目录，我们主要分析IPv4，所以还会涉及/net/ipv4/中的少量代码。\n接下来就以Intel e1000网卡为例，来一起探究网卡收包过程吧😀\ne1000 网卡 DMA 收包机制 e1000 网卡使用了 DMA（Direct Memory Access）：网卡硬件自己把数据写入内存中的接收缓冲区。那么Linux内核就要告诉网卡在DMA操作的时候把网络数据包copy到内存的哪个位置？这就需要网卡的驱动提前为网卡分配好专门保存数据包的内存，也就是ring buffer。\nRing Buffer 与描述符队列 驱动在初始化时，会为接收数据分配一块环形队列（Rx Ring Buffer）。\n队列中包含多个 接收描述符（Rx Descriptor）。 每个描述符里有： Buffer 地址（物理地址，告诉网卡数据该写到哪） Status 标志位（如 DD=1 表示数据已经写好） 网卡在收包时，就会：\n读取下一个描述符里的缓冲区地址 把数据包通过 DMA 写入这块内存 设置描述符状态（DD=1） 移动到下一个描述符 这样 CPU 完全不用参与搬运，只需在合适的时机读取描述符即可。\n源码片段：Rx Ring Buffer 初始化 以 Linux 4.4 内核的 e1000 驱动为例，接收队列初始化逻辑在 e1000_setup_rx_resources() 中（位于 drivers/net/ethernet/intel/e1000/e1000_main.c）。这段代码展示了给管理缓冲区的e1000_rx_ring结构体的成员赋值情况。\n// file: drivers/net/ethernet/intel/e1000/e1000_main.c /** * e1000_setup_rx_resources - allocate Rx resources (Descriptors) * @adapter: board private structure * @rxdr: rx descriptor ring (for a specific queue) to setup * * Returns 0 on success, negative on failure **/ static int e1000_setup_rx_resources(struct e1000_adapter *adapter, struct e1000_rx_ring *rxdr) { struct pci_dev *pdev = adapter-\u003epdev; int size, desc_len; size = sizeof(struct e1000_rx_buffer) * rxdr-\u003ecount; rxdr-\u003ebuffer_info = vzalloc(size);\t// 为buffer_info分配空间，buffer_info是一个数组，每个元素描述一个接收buffer if (!rxdr-\u003ebuffer_info) return -ENOMEM; desc_len = sizeof(struct e1000_rx_desc); /* Round up to nearest 4K */ rxdr-\u003esize = rxdr-\u003ecount * desc_len; rxdr-\u003esize = ALIGN(rxdr-\u003esize, 4096); rxdr-\u003edesc = dma_alloc_coherent(\u0026pdev-\u003edev, rxdr-\u003esize, \u0026rxdr-\u003edma, GFP_KERNEL); // 分配一块连续的物理内存用于存放描述符数组 if (!rxdr-\u003edesc) { setup_rx_desc_die: vfree(rxdr-\u003ebuffer_info); return -ENOMEM; } // ... memset(rxdr-\u003edesc, 0, rxdr-\u003esize);\t// 把所有文件描述符换清空 rxdr-\u003enext_to_clean = 0;\t// 下一个驱动要收回的描述符索引 rxdr-\u003enext_to_use = 0;\t// 下一个硬件可以用来DMA的描述符索引 rxdr-\u003erx_skb_top = NULL;\t// 用于大包接收时（分段skb）的临时缓存 return 0; } 看完这个函数不知道你是否有这样一个问题：这个函数申请了两块内存，一块是用来存放e1000_rx_buffer数组的，另一块是用来存放e1000_rx_desc数组的。这两个数组的元素都是用来描述真正的缓冲区的，但是这里偏偏没有分配缓冲区的操作。相反，还给e1000_rx_desc的所有元素都初始化为0了。其实申请缓冲区的操作在同一个文件的另一个函数e1000_alloc_rx_buffers中，这个函数主要工作就分配指定数量的buffer，并将buffer与e1000_rx_ring结构体的buffer_info和desc成员关联起来。\n// file: drivers/net/ethernet/intel/e1000/e1000_main.c /** * e1000_alloc_rx_buffers - Replace used receive buffers; legacy \u0026 extended * @adapter: address of board private structure **/ static void e1000_alloc_rx_buffers(struct e1000_adapter *adapter, struct e1000_rx_ring *rx_ring, int cleaned_count) { struct e1000_hw *hw = \u0026adapter-\u003ehw; struct pci_dev *pdev = adapter-\u003epdev; struct e1000_rx_desc *rx_desc; struct e1000_rx_buffer *buffer_info; unsigned int i; unsigned int bufsz = adapter-\u003erx_buffer_len; i = rx_ring-\u003enext_to_use; buffer_info = \u0026rx_ring-\u003ebuffer_info[i]; while (cleaned_count--) { void *data; if (buffer_info-\u003erxbuf.data) goto skip; // 为buffer分配内存 data = e1000_alloc_frag(adapter);\tif (!data) { /* Better luck next round */ adapter-\u003ealloc_rx_buff_failed++; break; } // ...... // 将buffer与buffer_info关联 buffer_info-\u003edma = dma_map_single(\u0026pdev-\u003edev, data, adapter-\u003erx_buffer_len, DMA_FROM_DEVICE);\tif (dma_mapping_error(\u0026pdev-\u003edev, buffer_info-\u003edma)) { skb_free_frag(data); buffer_info-\u003edma = 0; adapter-\u003ealloc_rx_buff_failed++; break; } buffer_info-\u003erxbuf.data = data; skip: // 将buffer与rx_desc关联 rx_desc = E1000_RX_DESC(*rx_ring, i); rx_desc-\u003ebuffer_addr = cpu_to_le64(buffer_info-\u003edma); if (unlikely(++i == rx_ring-\u003ecount)) i = 0; buffer_info = \u0026rx_ring-\u003ebuffer_info[i]; } if (likely(rx_ring-\u003enext_to_use != i)) { rx_ring-\u003enext_to_use = i; if (unlikely(i-- == 0)) i = (rx_ring-\u003ecount - 1); /* Force memory writes to complete before letting h/w * know there are new descriptors to fetch. (Only * applicable for weak-ordered memory model archs, * such as IA-64). */ wmb(); writel(i, hw-\u003ehw_addr + rx_ring-\u003erdt); } } 这就是网卡的接收环形缓冲区的初始化过程，初始化之后，网卡收到数据包之后就可以用DMA直接将数据包copy到对应的buffer中，并更新e1000_rx_ring的成员，然后就会触发中断，通知CPU有数据包到达。下面就该分析CPU是如何处理这个中断请求的。\nCPU 如何处理 e1000 网卡中断 当网卡接收到数据包并通过 DMA 写入内存后，会向 CPU 发出中断请求（IRQ），通知有新数据到达。Linux 内核中的 e1000 驱动会在中断向量表中注册自己的中断处理函数—— e1000_intr。\n中断处理函数运行在 硬中断上下文，主要完成三件事：\n读取并清除网卡的中断状态寄存器，确认事件类型（Rx、Tx、链路变化等）。 临时屏蔽中断，避免重复触发。 调度 NAPI poll，调用 napi_schedule(\u0026adapter-\u003enapi) 将网卡加入软中断处理队列。 NAPI（New API）是 Linux 的网络中断减载机制：在高流量下，它通过轮询（poll）批量处理数据包，而不是为每个数据包触发硬中断。\n// file: drivers/net/ethernet/intel/e1000/e1000_main.c /** * e1000_intr - Interrupt Handler * @irq: interrupt number * @data: pointer to a network interface device structure **/ static irqreturn_t e1000_intr(int irq, void *data) { struct net_device *netdev = data; struct e1000_adapter *adapter = netdev_priv(netdev); struct e1000_hw *hw = \u0026adapter-\u003ehw; u32 icr = er32(ICR); if (unlikely((!icr))) return IRQ_NONE; /* Not our interrupt */ /* we might have caused the interrupt, but the above * read cleared it, and just in case the driver is * down there is nothing to do so return handled */ if (unlikely(test_bit(__E1000_DOWN, \u0026adapter-\u003eflags))) return IRQ_HANDLED; if (unlikely(icr \u0026 (E1000_ICR_RXSEQ | E1000_ICR_LSC))) { hw-\u003eget_link_status = 1; /* guard against interrupt when we're going down */ if (!test_bit(__E1000_DOWN, \u0026adapter-\u003eflags)) schedule_delayed_work(\u0026adapter-\u003ewatchdog_task, 1); } /* disable interrupts, without the synchronize_irq bit */ ew32(IMC, ~0);\t// 禁止IMC（Interrupt Mask Clear）中断 E1000_WRITE_FLUSH(); if (likely(napi_schedule_prep(\u0026adapter-\u003enapi))) { adapter-\u003etotal_tx_bytes = 0; adapter-\u003etotal_tx_packets = 0; adapter-\u003etotal_rx_bytes = 0; adapter-\u003etotal_rx_packets = 0; __napi_schedule(\u0026adapter-\u003enapi);\t// 调度napi } else { /* this really should not happen! if it does it is basically a * bug, but not a hard error, so enable ints and continue */ if (!test_bit(__E1000_DOWN, \u0026adapter-\u003eflags)) e1000_irq_enable(adapter); } return IRQ_HANDLED; } e1000_intr做的事情是不是很少？这就对了，因为Linux内核处理硬件中断使用了上半部/下半部的两阶段机制，核心目的是既要尽快响应硬件，又要避免长时间占用CPU。上半部只做紧急的工作，如确认中断来源、通知硬件已收到通知、唤醒下半部等。e1000_intr就属于中断的上半部。\n软中断与 NAPI 轮询机制 在上一节中，我们看到网卡触发硬中断后，e1000 驱动的中断处理函数（e1000_intr）主要完成了 确认事件 → 临时屏蔽中断 → 调度 NAPI。那么接下来，真正的收包工作是如何展开的呢？这就涉及到 Linux 网络子系统里的 软中断 和 NAPI。\n硬中断到软中断的过渡 e1000_intr调用了__napi_schedule来调度NAPI。napi_schedule还会调用____napi_schedule。__napi_schedule()的操作就是：\n把当前网卡对应的 napi_struct 对象加入到全局的 NAPI 队列； 设置 NET_RX_SOFTIRQ 标志，告诉内核后续需要执行网络接收的软中断。 这样，真正的数据包处理就被延迟到了软中断上下文执行，避免在硬中断上下文里做大量工作。\n// file: net/core/dev.c /* Called with irq disabled */ static inline void ____napi_schedule(struct softnet_data *sd, struct napi_struct *napi) { list_add_tail(\u0026napi-\u003epoll_list, \u0026sd-\u003epoll_list);\t// 把napi_struct对象加入到全局的NAPI队列 __raise_softirq_irqoff(NET_RX_SOFTIRQ);\t// 设置NET_RX_SOFTIRQ标志，告诉内核后续要执行网络接收的软中断 } void __napi_schedule(struct napi_struct *n) { unsigned long flags; local_irq_save(flags); ____napi_schedule(this_cpu_ptr(\u0026softnet_data), n); // 调用____napi_schedule local_irq_restore(flags); } EXPORT_SYMBOL(__napi_schedule); 软中断由 do_softirq() 执行，其中会调用 net_rx_action()，这是因为net_dev在初始化的时候会注册NET_RX_SOFTIRQ和NET_TX_SOFTIRQ软中断。\n// file: net/core/dev.c /* *\tInitialize the DEV module. At boot time this walks the device list and *\tunhooks any devices that fail to initialise (normally hardware not *\tpresent) and leaves us with a valid list of present and active devices. * */ /* * This is called single threaded during boot, so no need * to take the rtnl semaphore. */ static int __init net_dev_init(void) { int i, rc = -ENOMEM; // ...... open_softirq(NET_TX_SOFTIRQ, net_tx_action);\t// 指定由 net_tx_action 处理 NET_TX_SOFTIRQ 软中断 open_softirq(NET_RX_SOFTIRQ, net_rx_action);\t// 指定由 net_rx_action 处理 NET_RX_SOFTIRQ 软中断 // ...... return rc; } subsys_initcall(net_dev_init); net_rx_action() 的职责是轮询 NAPI 队列，逐个调用设备驱动注册的 poll 方法。 对 e1000 驱动来说，这个 poll 函数就是 e1000_clean()。\n// file: net/core/dev.c static void net_rx_action(struct softirq_action *h) { // ...... for (;;) { struct napi_struct *n; // ...... n = list_first_entry(\u0026list, struct napi_struct, poll_list);\t// 从napi struct队列中取出napi对象 budget -= napi_poll(n, \u0026repoll);\t// 将对象交给 napi_poll 处理，对于e1000驱动来说，poll就是e1000_clean // ...... } // ...... } static int napi_poll(struct napi_struct *n, struct list_head *repoll) { void *have; int work, weight; // ...... work = 0; if (test_bit(NAPI_STATE_SCHED, \u0026n-\u003estate)) { work = n-\u003epoll(n, weight);\t// 调用 napi struct 对象的 poll 函数处理 trace_napi_poll(n); } // ...... return work; } 最终软中断还是调用了e1000网卡的驱动函数e1000_clean。e1000_clean的职责是：在软中断上下文中处理收包与发包的完成逻辑。这里我们只关注收包，忽略发包。发包的逻辑实际是调用adapter-\u003eclean_rx来完成的。\n// file: drivers/net/ethernet/intel/e1000/e1000_main.c /** * e1000_clean - NAPI Rx polling callback * @adapter: board private structure **/ static int e1000_clean(struct napi_struct *napi, int budget) { struct e1000_adapter *adapter = container_of(napi, struct e1000_adapter, napi); int tx_clean_complete = 0, work_done = 0; /* 1. 回收发送队列 */ tx_clean_complete = e1000_clean_tx_irq(adapter, \u0026adapter-\u003etx_ring[0]); /* 2. 处理接收队列 */ adapter-\u003eclean_rx(adapter, \u0026adapter-\u003erx_ring[0], \u0026work_done, budget); /* 3. 如果发送队列没清理干净，强制认为收包已经达到 budget */ if (!tx_clean_complete) work_done = budget; /* 4. 如果收发工作没用完 budget，说明任务完成，可以退出 NAPI 轮询 */ /* If budget not fully consumed, exit the polling mode */ if (work_done \u003c budget) { if (likely(adapter-\u003eitr_setting \u0026 3)) e1000_set_itr(adapter);\t// 动态调整中断频率 napi_complete_done(napi, work_done);\t// 标记 NAPI 处理完成 if (!test_bit(__E1000_DOWN, \u0026adapter-\u003eflags)) e1000_irq_enable(adapter);\t// 重新开启中断 } return work_done; } 为什么发包直接调用e1000_clean_tx_irq，而收报不是直接调用e1000_clean_rx_irq呢？这是因为处理收包的函数有多个，具体使用哪个是由配置决定的。具体来说就是，如果MTU大于标准以太网数据长度ETH_DATA_LEN（1500 字节）就使用专门处理巨帧的函数，否则就使用普通的收包函数。后面我们以普通收包函数（e1000_clean_rx_irq）为例进行分析。\n// file: drivers/net/ethernet/intel/e1000/e1000_main.c static void e1000_configure_rx(struct e1000_adapter *adapter) { // ...... if (adapter-\u003enetdev-\u003emtu \u003e ETH_DATA_LEN) { // 检查 mtu 是否大于 ETH_DATA_LEN(1500) rdlen = adapter-\u003erx_ring[0].count * sizeof(struct e1000_rx_desc); adapter-\u003eclean_rx = e1000_clean_jumbo_rx_irq;\t// 使用专门处理jumbo frame的收包函数 adapter-\u003ealloc_rx_buf = e1000_alloc_jumbo_rx_buffers; } else { rdlen = adapter-\u003erx_ring[0].count * sizeof(struct e1000_rx_desc); adapter-\u003eclean_rx = e1000_clean_rx_irq;\t// 使用普通收包函数 adapter-\u003ealloc_rx_buf = e1000_alloc_rx_buffers; } // ...... } 概括来说e1000_clean_rx_irq从DMA ring取出报文，封装成skb，然后交给e1000_receive_skb处理。具体包括：\n检查 DMA 完成标志\n每个 rx_desc（接收描述符）在网卡写完数据后，会设置 E1000_RXD_STAT_DD。 驱动只处理已经 DMA 完成的 buffer。 构造 skb\n小包：调用 e1000_copybreak()，复制数据到新建的 skb。 大包：直接用 build_skb()，避免复制开销，直接复用 DMA buffer。 丢弃非法数据包\n如果包跨越多个 buffer（EOP 未置位），直接丢弃。 硬件发现的错误帧（CRC 错误等）也会丢弃。 合法数据包处理\n调整 skb 长度，去掉以太网 CRC。 调用 e1000_rx_checksum() 做硬件校验和 offload。 调用 e1000_receive_skb() 把报文交给 Linux 网络协议栈（从这一刻开始，数据包就进入了 TCP/IP 栈）。 回收和补充 buffer\n清空 rx_desc-\u003estatus，表示这个描述符可以再次被网卡使用。 定期调用 alloc_rx_buf() 给网卡补充新的空 buffer，避免 ring buffer 被耗尽。 // file: drivers/net/ethernet/intel/e1000/e1000_main.c /** * e1000_clean_rx_irq - Send received data up the network stack; legacy * @adapter: board private structure * @rx_ring: ring to clean * @work_done: amount of napi work completed this call * @work_to_do: max amount of work allowed for this call to do */ static bool e1000_clean_rx_irq(struct e1000_adapter *adapter, struct e1000_rx_ring *rx_ring, int *work_done, int work_to_do) { struct net_device *netdev = adapter-\u003enetdev; struct pci_dev *pdev = adapter-\u003epdev; struct e1000_rx_desc *rx_desc, *next_rxd; struct e1000_rx_buffer *buffer_info, *next_buffer; u32 length; unsigned int i; int cleaned_count = 0; bool cleaned = false; unsigned int total_rx_bytes=0, total_rx_packets=0; i = rx_ring-\u003enext_to_clean; rx_desc = E1000_RX_DESC(*rx_ring, i); buffer_info = \u0026rx_ring-\u003ebuffer_info[i]; while (rx_desc-\u003estatus \u0026 E1000_RXD_STAT_DD) {\t// 检查DMA是否完成，DD = Descriptor Done struct sk_buff *skb; u8 *data; u8 status; if (*work_done \u003e= work_to_do) break; (*work_done)++; dma_rmb(); /* read descriptor and rx_buffer_info after status DD */ status = rx_desc-\u003estatus; length = le16_to_cpu(rx_desc-\u003elength); data = buffer_info-\u003erxbuf.data;\t// data就是真正的buffer prefetch(data); // 构造skb，如果是小包，直接把数据包内从从buffer中copy一份，否则直接使用buffer skb = e1000_copybreak(adapter, buffer_info, length, data); // 为小包分配skb，如果是大包，返回NULL if (!skb) {\t// 处理大包 unsigned int frag_len = e1000_frag_len(adapter); skb = build_skb(data - E1000_HEADROOM, frag_len); // 为大包构造skb // ...... } // ...... process_skb: total_rx_bytes += (length - 4); /* don't count FCS */ total_rx_packets++; if (likely(!(netdev-\u003efeatures \u0026 NETIF_F_RXFCS))) /* adjust length to remove Ethernet CRC, this must be * done after the TBI_ACCEPT workaround above */ length -= 4; // 调整skb if (buffer_info-\u003erxbuf.data == NULL) skb_put(skb, length); else /* copybreak skb */ skb_trim(skb, length); // /* Receive Checksum Offload */ e1000_rx_checksum(adapter, (u32)(status) | ((u32)(rx_desc-\u003eerrors) \u003c\u003c 24), le16_to_cpu(rx_desc-\u003ecsum), skb); // 交给e1000_receive_skb处理 e1000_receive_skb(adapter, status, rx_desc-\u003especial, skb); // ...... } // ...... // 更新统计信息 adapter-\u003etotal_rx_packets += total_rx_packets; adapter-\u003etotal_rx_bytes += total_rx_bytes; netdev-\u003estats.rx_bytes += total_rx_bytes; netdev-\u003estats.rx_packets += total_rx_packets; return cleaned; } e1000_receive_skb根据协议（对于e1000网卡来说，就是以太网协议）调整skb的相关字段（如mac_header、data等），然后将skb交给napi_gro_receive处理。\n// file: drivers/net/ethernet/intel/e1000/e1000_main.c /** * e1000_receive_skb - helper function to handle rx indications * @adapter: board private structure * @status: descriptor status field as written by hardware * @vlan: descriptor vlan field as written by hardware (no le/be conversion) * @skb: pointer to sk_buff to be indicated to stack */ static void e1000_receive_skb(struct e1000_adapter *adapter, u8 status, __le16 vlan, struct sk_buff *skb) { skb-\u003eprotocol = eth_type_trans(skb, adapter-\u003enetdev);\t// 调整skb的mac_header，根据L2 header判断协议类型，并将skb-\u003edata指向下一层协议头 if (status \u0026 E1000_RXD_STAT_VP) { u16 vid = le16_to_cpu(vlan) \u0026 E1000_RXD_SPC_VLAN_MASK; __vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vid); } napi_gro_receive(\u0026adapter-\u003enapi, skb);\t// 交给上层协议栈处理 } napi_gro_receive是Linux内核GRO（Generic Receive Offload）接收路径的一个函数，实现了对接收到的skb进行聚合处理。这里我们不关心具体的聚合操作细节（只需要知道dev_gro_receive会给napi_skb_finish返回聚合的结果即可），直接看napi_skb_finish是如何处理skb的。napi_skb_finish根据聚合结果判断是否继续处理skb，正常情况下会将skb交给其他函数处理，最终skb会被__netif_receive_skb_core处理。\n// file: net/core/dev.c gro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb) { trace_napi_gro_receive_entry(skb);\t// 追踪调试入口，可用于ftrace等跟踪工具 skb_gro_reset_offset(skb);\t// 重置 skb 的 GRO 状态，让 skb 可以参与聚合 // 调用 dev_gro_receive 将 skb 交给设备层的 GRO， // dev_gro_receive 会尝试将多个小包聚合成大包，提高处理效率 // napi_skb_finish 根据 dev_gro_receive 的返回结果做后续处理 return napi_skb_finish(dev_gro_receive(napi, skb), skb); } EXPORT_SYMBOL(napi_gro_receive); __netif_receive_skb_core比较长，以下是保留了skb分发逻辑的简化版本。主要流程如下：\n先分发给全局链表 ptype_all（tcpdump 等监听工具会在这里收到数据包）。\n再分发给网卡专属链表 dev-\u003eptype_all（网卡功能相关）。\n调用网卡 rx_handler（驱动自定义的接收回调）。\n根据协议类型分发给 IPv4/IPv6/ARP等处理。\n最后调用最后一个 packet_type 的处理函数或丢弃。\n对于IP数据包，就会分发给ip_rcv函数处理。至此网络数据包就从网卡到达到了网络协议栈的入口。\n// file: net/core/dev.c static int __netif_receive_skb_core(struct sk_buff *skb) { struct packet_type *ptype, *pt_prev; rx_handler_func_t *rx_handler; struct net_device *orig_dev; int ret = NET_RX_DROP; __be16 type; orig_dev = skb-\u003edev; pt_prev = NULL; // 1. 分发给全局 packet_type 链表（所有网卡共享） list_for_each_entry_rcu(ptype, \u0026ptype_all, list) { if (pt_prev) deliver_skb(skb, pt_prev, orig_dev); pt_prev = ptype; } // 2. 分发给当前网卡专属 packet_type 链表 list_for_each_entry_rcu(ptype, \u0026skb-\u003edev-\u003eptype_all, list) { if (pt_prev) deliver_skb(skb, pt_prev, orig_dev); pt_prev = ptype; } // 3. 调用网卡驱动可能设置的 rx_handler rx_handler = rcu_dereference(skb-\u003edev-\u003erx_handler); if (rx_handler) { if (pt_prev) { deliver_skb(skb, pt_prev, orig_dev); pt_prev = NULL; } switch (rx_handler(\u0026skb)) { case RX_HANDLER_CONSUMED: return NET_RX_SUCCESS; case RX_HANDLER_ANOTHER: // skb 被重定向，需要再次分发 pt_prev = NULL; break; case RX_HANDLER_EXACT: case RX_HANDLER_PASS: break; } } // 4. 按协议类型分发给对应链表（IPv4/IPv6/ARP） type = skb-\u003eprotocol; deliver_ptype_list_skb(skb, \u0026pt_prev, orig_dev, type, \u0026ptype_base[ntohs(type) \u0026 PTYPE_HASH_MASK]);\t// 对于TCP/UDP，就是在这里分发给IPv4处理的 deliver_ptype_list_skb(skb, \u0026pt_prev, orig_dev, type, \u0026orig_dev-\u003eptype_specific); // 5. 最终调用最后一个 packet_type 的处理函数，或者丢弃 if (pt_prev) ret = pt_prev-\u003efunc(skb, skb-\u003edev, pt_prev, orig_dev); else { kfree_skb(skb); ret = NET_RX_DROP; } return ret; } 这里插播一段内容：tcpdump的抓包的实现原理其实就是创建PF_PACKET类型的socket并添加到ptype_all或者dev-\u003eptype_all，而每一个数据包都会经历前面的1、2两个分发过程。可以看到tcpdump的抓包位置在数据包处理的早期（入向流量），所以即使数据包会被丢弃，tcpdump也能抓到完整的数据包。\n如何确定把数据包分发给谁处理呢？对于IPv4数据包而言，e1000网卡在将skb交给napi_gro_receive前，会设置skb-\u003eprotocol，__netif_receive_skb_core在分发的时候会在ptype_base中查找匹配的处理函数，对于IPv4而言，在初始化的时候会将ip_packet_type添加到ptype_base中。\n// file: net/ipv4/af_inet.c static struct packet_type ip_packet_type __read_mostly = { .type = cpu_to_be16(ETH_P_IP), .func = ip_rcv,\t// 处理IP数据包的函数 }; static int __init inet_init(void) { // ...... dev_add_pack(\u0026ip_packet_type); // ...... } 总结 经过一通分析，终于把数据包从网卡送到了内核网络协议栈，还意外的发现了tcpdump抓包的位置（入向包），真不容易。回过头来再看这个过程：\n数据包达到网卡； 网卡通过DMA将数据包写入驱动预先分配好的接收缓冲区； 网卡通过IRQ告诉CPU有新的网络数据包到达； CPU快速响应网卡的IRQ请求，简单调用__napi_schedule()，把NAPI poll加入调度队列； 软中断调度NAPI poll，最终调用e1000网卡的poll函数（e1000_clean）； 网卡的poll函读取DMA ring描述符，把数据包封装进sk_buff结构体（即skb），交给内核网络子系统； 内核网络子系统按照协议类型将skb分发给对应的处理函数，对于IP数据包而言，处理函数就是ip_rcv； 在分析的过程中，我们还看到了在分发skb时会遍历ptype_all，如果使用tcpdump抓包，tcpdump创建的socket就会被添加到ptype_all中，所以就能抓到L2~L7的完整数据包。\nip_rcv如何处理数据包的呢？这涉及到路由以及分发给L4层处理（如TCP/UDP）。这块内容留在下一篇文章介绍。\n","wordCount":"1780","inLanguage":"zh","datePublished":"2025-09-22T20:43:51+08:00","dateModified":"2025-09-22T20:43:51+08:00","author":{"@type":"Person","name":"rand0m"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://rand0m42195.github.io/posts/linux-networking-receive/"},"publisher":{"@type":"Organization","name":"rand0m's blog","logo":{"@type":"ImageObject","url":"https://rand0m42195.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rand0m42195.github.io/ accesskey=h title="rand0m's blog (Alt + H)">rand0m's blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://rand0m42195.github.io/posts/ title=文章><span>文章</span></a></li><li><a href=https://rand0m42195.github.io/about/ title=关于><span>关于</span></a></li><li><a href=https://github.com/rand0m42195 title=GitHub><span>GitHub</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://rand0m42195.github.io/>主页</a>&nbsp;»&nbsp;<a href=https://rand0m42195.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">网络数据包接受过程分析——从网卡到内核协议栈（以Intel e1000 + Linux 4.4为例）</h1><div class=post-meta><span title='2025-09-22 20:43:51 +0800 +0800'>九月 22, 2025</span>&nbsp;·&nbsp;9 分钟&nbsp;·&nbsp;rand0m</div></header><div class=post-content><h1 id=引言>引言<a hidden class=anchor aria-hidden=true href=#引言>#</a></h1><p>网络数据包从网卡到应用程序，需要经历一段复杂的旅程。作为开发者，我们平时调用 <code>socket()</code>、<code>recv()</code> 就能轻松拿到数据，却很少思考内核背后究竟发生了什么。</p><p>本系列文章尝试结合 <strong>理论流程 + 内核源码分析</strong>，逐步剖析 Linux 内核中网络数据包的接收过程。这里选择 Linux 4.4 内核作为例子（代码相对稳定，资料丰富，逻辑上没有过多新特性干扰），并结合 Intel e1000 驱动来具体展示数据包是如何从网卡到达内核网络协议栈的。</p><h1 id=网络数据包接收的总体流程>网络数据包接收的总体流程<a hidden class=anchor aria-hidden=true href=#网络数据包接收的总体流程>#</a></h1><p>先给出一个全局视角：数据包从网卡到达内存，再到协议栈的路径，大致如下：</p><ol><li><strong>数据包到达网卡</strong>
网卡硬件接收以太帧，做基本校验（如 CRC）。</li><li><strong>DMA 写入内存</strong>
网卡通过 DMA 将数据包写入驱动预先分配好的接收缓冲区（Descriptor Ring）。</li><li><strong>中断通知 CPU</strong>
网卡通过 IRQ 告诉 CPU：“我收到了新数据包”。</li><li><strong>驱动中断处理函数（ISR）</strong>
驱动快速处理中断，通常只是调用 <code>__napi_schedule()</code>，把 NAPI poll 加入调度队列。</li><li><strong>软中断调度 NAPI poll</strong>
CPU 执行 <code>do_softirq()</code> → <code>net_rx_action()</code> → 调用 e1000 的 poll 函数。</li><li><strong>poll 函数提取数据包并构造 skb</strong>
驱动在 poll 中读取 DMA ring 的描述符，把数据包封装进 <code>sk_buff</code> 结构，交给内核网络子系统。</li><li><strong>协议栈处理</strong>
skb 被送到 IP 层，进一步交给 TCP/UDP，最终到达 socket，供应用程序读取。</li></ol><p><img alt=网卡收包流程 loading=lazy src=/images/posts/linux-networking-receive/nic-networking-stack.png></p><p>在开始具体分析之前，先说以下相关源码的位置。和网卡相关的代码在驱动目录下（<code>/drivers/net/ethernet</code>），由于我们分析的是Intel的e1000网卡，所以具体位置就是<code>/drivers/net/ethernet/intel/e1000</code>，内核网络协议栈位于网络子系统目录下<code>/net</code>，主要是<code>/net/core/</code>目录，我们主要分析IPv4，所以还会涉及<code>/net/ipv4/</code>中的少量代码。</p><p>接下来就以Intel e1000网卡为例，来一起探究网卡收包过程吧😀</p><h2 id=e1000-网卡-dma-收包机制>e1000 网卡 DMA 收包机制<a hidden class=anchor aria-hidden=true href=#e1000-网卡-dma-收包机制>#</a></h2><p>e1000 网卡使用了 <strong>DMA（Direct Memory Access）</strong>：网卡硬件自己把数据写入内存中的接收缓冲区。那么Linux内核就要告诉网卡在DMA操作的时候把网络数据包copy到内存的哪个位置？这就需要网卡的驱动提前为网卡分配好专门保存数据包的内存，也就是ring buffer。</p><h3 id=ring-buffer-与描述符队列>Ring Buffer 与描述符队列<a hidden class=anchor aria-hidden=true href=#ring-buffer-与描述符队列>#</a></h3><p>驱动在初始化时，会为接收数据分配一块环形队列（Rx Ring Buffer）。</p><ul><li>队列中包含多个 <strong>接收描述符（Rx Descriptor）</strong>。</li><li>每个描述符里有：<ul><li><strong>Buffer 地址</strong>（物理地址，告诉网卡数据该写到哪）</li><li><strong>Status 标志位</strong>（如 DD=1 表示数据已经写好）</li></ul></li></ul><p>网卡在收包时，就会：</p><ol><li>读取下一个描述符里的缓冲区地址</li><li>把数据包通过 DMA 写入这块内存</li><li>设置描述符状态（DD=1）</li><li>移动到下一个描述符</li></ol><p>这样 CPU 完全不用参与搬运，只需在合适的时机读取描述符即可。</p><h3 id=源码片段rx-ring-buffer-初始化>源码片段：Rx Ring Buffer 初始化<a hidden class=anchor aria-hidden=true href=#源码片段rx-ring-buffer-初始化>#</a></h3><p>以 Linux 4.4 内核的 e1000 驱动为例，接收队列初始化逻辑在 <code>e1000_setup_rx_resources()</code> 中（位于 <code>drivers/net/ethernet/intel/e1000/e1000_main.c</code>）。这段代码展示了给管理缓冲区的<code>e1000_rx_ring</code>结构体的成员赋值情况。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#75715e>// file: drivers/net/ethernet/intel/e1000/e1000_main.c
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>/**
</span></span></span><span style=display:flex><span><span style=color:#75715e> * e1000_setup_rx_resources - allocate Rx resources (Descriptors)
</span></span></span><span style=display:flex><span><span style=color:#75715e> * @adapter: board private structure
</span></span></span><span style=display:flex><span><span style=color:#75715e> * @rxdr:    rx descriptor ring (for a specific queue) to setup
</span></span></span><span style=display:flex><span><span style=color:#75715e> *
</span></span></span><span style=display:flex><span><span style=color:#75715e> * Returns 0 on success, negative on failure
</span></span></span><span style=display:flex><span><span style=color:#75715e> **/</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>int</span> <span style=color:#a6e22e>e1000_setup_rx_resources</span>(<span style=color:#66d9ef>struct</span> e1000_adapter <span style=color:#f92672>*</span>adapter,
</span></span><span style=display:flex><span>				    <span style=color:#66d9ef>struct</span> e1000_rx_ring <span style=color:#f92672>*</span>rxdr)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>struct</span> pci_dev <span style=color:#f92672>*</span>pdev <span style=color:#f92672>=</span> adapter<span style=color:#f92672>-&gt;</span>pdev;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>int</span> size, desc_len;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	size <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>struct</span> e1000_rx_buffer) <span style=color:#f92672>*</span> rxdr<span style=color:#f92672>-&gt;</span>count;
</span></span><span style=display:flex><span>	rxdr<span style=color:#f92672>-&gt;</span>buffer_info <span style=color:#f92672>=</span> <span style=color:#a6e22e>vzalloc</span>(size);	<span style=color:#75715e>// 为buffer_info分配空间，buffer_info是一个数组，每个元素描述一个接收buffer
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>rxdr<span style=color:#f92672>-&gt;</span>buffer_info)
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span>ENOMEM;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	desc_len <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>struct</span> e1000_rx_desc);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>/* Round up to nearest 4K */</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	rxdr<span style=color:#f92672>-&gt;</span>size <span style=color:#f92672>=</span> rxdr<span style=color:#f92672>-&gt;</span>count <span style=color:#f92672>*</span> desc_len;
</span></span><span style=display:flex><span>	rxdr<span style=color:#f92672>-&gt;</span>size <span style=color:#f92672>=</span> <span style=color:#a6e22e>ALIGN</span>(rxdr<span style=color:#f92672>-&gt;</span>size, <span style=color:#ae81ff>4096</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	rxdr<span style=color:#f92672>-&gt;</span>desc <span style=color:#f92672>=</span> <span style=color:#a6e22e>dma_alloc_coherent</span>(<span style=color:#f92672>&amp;</span>pdev<span style=color:#f92672>-&gt;</span>dev, rxdr<span style=color:#f92672>-&gt;</span>size, <span style=color:#f92672>&amp;</span>rxdr<span style=color:#f92672>-&gt;</span>dma,
</span></span><span style=display:flex><span>					GFP_KERNEL); <span style=color:#75715e>// 分配一块连续的物理内存用于存放描述符数组
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>rxdr<span style=color:#f92672>-&gt;</span>desc) {
</span></span><span style=display:flex><span>setup_rx_desc_die:
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>vfree</span>(rxdr<span style=color:#f92672>-&gt;</span>buffer_info);
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span>ENOMEM;
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>// ...
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#a6e22e>memset</span>(rxdr<span style=color:#f92672>-&gt;</span>desc, <span style=color:#ae81ff>0</span>, rxdr<span style=color:#f92672>-&gt;</span>size);	<span style=color:#75715e>// 把所有文件描述符换清空
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>	rxdr<span style=color:#f92672>-&gt;</span>next_to_clean <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;	<span style=color:#75715e>// 下一个驱动要收回的描述符索引
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	rxdr<span style=color:#f92672>-&gt;</span>next_to_use <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;		<span style=color:#75715e>// 下一个硬件可以用来DMA的描述符索引
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	rxdr<span style=color:#f92672>-&gt;</span>rx_skb_top <span style=color:#f92672>=</span> NULL;	<span style=color:#75715e>// 用于大包接收时（分段skb）的临时缓存 
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>看完这个函数不知道你是否有这样一个问题：这个函数申请了两块内存，一块是用来存放<code>e1000_rx_buffer</code>数组的，另一块是用来存放<code>e1000_rx_desc</code>数组的。这两个数组的元素都是用来描述真正的缓冲区的，但是这里偏偏没有分配缓冲区的操作。相反，还给<code>e1000_rx_desc</code>的所有元素都初始化为0了。其实申请缓冲区的操作在同一个文件的另一个函数<code>e1000_alloc_rx_buffers</code>中，这个函数主要工作就分配指定数量的buffer，并将buffer与<code>e1000_rx_ring</code>结构体的<code>buffer_info</code>和<code>desc</code>成员关联起来。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#75715e>// file: drivers/net/ethernet/intel/e1000/e1000_main.c
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>/**
</span></span></span><span style=display:flex><span><span style=color:#75715e> * e1000_alloc_rx_buffers - Replace used receive buffers; legacy &amp; extended
</span></span></span><span style=display:flex><span><span style=color:#75715e> * @adapter: address of board private structure
</span></span></span><span style=display:flex><span><span style=color:#75715e> **/</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>e1000_alloc_rx_buffers</span>(<span style=color:#66d9ef>struct</span> e1000_adapter <span style=color:#f92672>*</span>adapter,
</span></span><span style=display:flex><span>				   <span style=color:#66d9ef>struct</span> e1000_rx_ring <span style=color:#f92672>*</span>rx_ring,
</span></span><span style=display:flex><span>				   <span style=color:#66d9ef>int</span> cleaned_count)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>struct</span> e1000_hw <span style=color:#f92672>*</span>hw <span style=color:#f92672>=</span> <span style=color:#f92672>&amp;</span>adapter<span style=color:#f92672>-&gt;</span>hw;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>struct</span> pci_dev <span style=color:#f92672>*</span>pdev <span style=color:#f92672>=</span> adapter<span style=color:#f92672>-&gt;</span>pdev;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>struct</span> e1000_rx_desc <span style=color:#f92672>*</span>rx_desc;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>struct</span> e1000_rx_buffer <span style=color:#f92672>*</span>buffer_info;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>int</span> i;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>int</span> bufsz <span style=color:#f92672>=</span> adapter<span style=color:#f92672>-&gt;</span>rx_buffer_len;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	i <span style=color:#f92672>=</span> rx_ring<span style=color:#f92672>-&gt;</span>next_to_use;
</span></span><span style=display:flex><span>	buffer_info <span style=color:#f92672>=</span> <span style=color:#f92672>&amp;</span>rx_ring<span style=color:#f92672>-&gt;</span>buffer_info[i];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>while</span> (cleaned_count<span style=color:#f92672>--</span>) {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>void</span> <span style=color:#f92672>*</span>data;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> (buffer_info<span style=color:#f92672>-&gt;</span>rxbuf.data)
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>goto</span> skip;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>// 为buffer分配内存
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		data <span style=color:#f92672>=</span> <span style=color:#a6e22e>e1000_alloc_frag</span>(adapter);	
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>data) {
</span></span><span style=display:flex><span>			<span style=color:#75715e>/* Better luck next round */</span>
</span></span><span style=display:flex><span>			adapter<span style=color:#f92672>-&gt;</span>alloc_rx_buff_failed<span style=color:#f92672>++</span>;
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>break</span>;
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e>// 将buffer与buffer_info关联
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		buffer_info<span style=color:#f92672>-&gt;</span>dma <span style=color:#f92672>=</span> <span style=color:#a6e22e>dma_map_single</span>(<span style=color:#f92672>&amp;</span>pdev<span style=color:#f92672>-&gt;</span>dev,
</span></span><span style=display:flex><span>						  data,
</span></span><span style=display:flex><span>						  adapter<span style=color:#f92672>-&gt;</span>rx_buffer_len,
</span></span><span style=display:flex><span>						  DMA_FROM_DEVICE);	
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>dma_mapping_error</span>(<span style=color:#f92672>&amp;</span>pdev<span style=color:#f92672>-&gt;</span>dev, buffer_info<span style=color:#f92672>-&gt;</span>dma)) {
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>skb_free_frag</span>(data);
</span></span><span style=display:flex><span>			buffer_info<span style=color:#f92672>-&gt;</span>dma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>			adapter<span style=color:#f92672>-&gt;</span>alloc_rx_buff_failed<span style=color:#f92672>++</span>;
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>break</span>;
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		buffer_info<span style=color:#f92672>-&gt;</span>rxbuf.data <span style=color:#f92672>=</span> data;
</span></span><span style=display:flex><span> skip:
</span></span><span style=display:flex><span>        <span style=color:#75715e>// 将buffer与rx_desc关联
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		rx_desc <span style=color:#f92672>=</span> <span style=color:#a6e22e>E1000_RX_DESC</span>(<span style=color:#f92672>*</span>rx_ring, i);
</span></span><span style=display:flex><span>		rx_desc<span style=color:#f92672>-&gt;</span>buffer_addr <span style=color:#f92672>=</span> <span style=color:#a6e22e>cpu_to_le64</span>(buffer_info<span style=color:#f92672>-&gt;</span>dma);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>unlikely</span>(<span style=color:#f92672>++</span>i <span style=color:#f92672>==</span> rx_ring<span style=color:#f92672>-&gt;</span>count))
</span></span><span style=display:flex><span>			i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>		buffer_info <span style=color:#f92672>=</span> <span style=color:#f92672>&amp;</span>rx_ring<span style=color:#f92672>-&gt;</span>buffer_info[i];
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>likely</span>(rx_ring<span style=color:#f92672>-&gt;</span>next_to_use <span style=color:#f92672>!=</span> i)) {
</span></span><span style=display:flex><span>		rx_ring<span style=color:#f92672>-&gt;</span>next_to_use <span style=color:#f92672>=</span> i;
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>unlikely</span>(i<span style=color:#f92672>--</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>))
</span></span><span style=display:flex><span>			i <span style=color:#f92672>=</span> (rx_ring<span style=color:#f92672>-&gt;</span>count <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#75715e>/* Force memory writes to complete before letting h/w
</span></span></span><span style=display:flex><span><span style=color:#75715e>		 * know there are new descriptors to fetch.  (Only
</span></span></span><span style=display:flex><span><span style=color:#75715e>		 * applicable for weak-ordered memory model archs,
</span></span></span><span style=display:flex><span><span style=color:#75715e>		 * such as IA-64).
</span></span></span><span style=display:flex><span><span style=color:#75715e>		 */</span>
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>wmb</span>();
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>writel</span>(i, hw<span style=color:#f92672>-&gt;</span>hw_addr <span style=color:#f92672>+</span> rx_ring<span style=color:#f92672>-&gt;</span>rdt);
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>这就是网卡的接收环形缓冲区的初始化过程，初始化之后，网卡收到数据包之后就可以用DMA直接将数据包copy到对应的buffer中，并更新<code>e1000_rx_ring</code>的成员，然后就会触发中断，通知CPU有数据包到达。下面就该分析CPU是如何处理这个中断请求的。</p><h3 id=cpu-如何处理-e1000-网卡中断>CPU 如何处理 e1000 网卡中断<a hidden class=anchor aria-hidden=true href=#cpu-如何处理-e1000-网卡中断>#</a></h3><p>当网卡接收到数据包并通过 DMA 写入内存后，会向 CPU 发出中断请求（IRQ），通知有新数据到达。Linux 内核中的 e1000 驱动会在中断向量表中注册自己的中断处理函数—— <code>e1000_intr</code>。</p><p>中断处理函数运行在 <strong>硬中断上下文</strong>，主要完成三件事：</p><ol><li>读取并清除网卡的中断状态寄存器，确认事件类型（Rx、Tx、链路变化等）。</li><li>临时屏蔽中断，避免重复触发。</li><li>调度 NAPI poll，调用 <code>napi_schedule(&amp;adapter->napi)</code> 将网卡加入软中断处理队列。</li></ol><p>NAPI（New API）是 Linux 的网络中断减载机制：在高流量下，它通过轮询（poll）批量处理数据包，而不是为每个数据包触发硬中断。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#75715e>// file: drivers/net/ethernet/intel/e1000/e1000_main.c
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>/**
</span></span></span><span style=display:flex><span><span style=color:#75715e> * e1000_intr - Interrupt Handler
</span></span></span><span style=display:flex><span><span style=color:#75715e> * @irq: interrupt number
</span></span></span><span style=display:flex><span><span style=color:#75715e> * @data: pointer to a network interface device structure
</span></span></span><span style=display:flex><span><span style=color:#75715e> **/</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>irqreturn_t</span> <span style=color:#a6e22e>e1000_intr</span>(<span style=color:#66d9ef>int</span> irq, <span style=color:#66d9ef>void</span> <span style=color:#f92672>*</span>data)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>struct</span> net_device <span style=color:#f92672>*</span>netdev <span style=color:#f92672>=</span> data;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>struct</span> e1000_adapter <span style=color:#f92672>*</span>adapter <span style=color:#f92672>=</span> <span style=color:#a6e22e>netdev_priv</span>(netdev);
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>struct</span> e1000_hw <span style=color:#f92672>*</span>hw <span style=color:#f92672>=</span> <span style=color:#f92672>&amp;</span>adapter<span style=color:#f92672>-&gt;</span>hw;
</span></span><span style=display:flex><span>	u32 icr <span style=color:#f92672>=</span> <span style=color:#a6e22e>er32</span>(ICR);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>unlikely</span>((<span style=color:#f92672>!</span>icr)))
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>return</span> IRQ_NONE;  <span style=color:#75715e>/* Not our interrupt */</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>/* we might have caused the interrupt, but the above
</span></span></span><span style=display:flex><span><span style=color:#75715e>	 * read cleared it, and just in case the driver is
</span></span></span><span style=display:flex><span><span style=color:#75715e>	 * down there is nothing to do so return handled
</span></span></span><span style=display:flex><span><span style=color:#75715e>	 */</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>unlikely</span>(<span style=color:#a6e22e>test_bit</span>(__E1000_DOWN, <span style=color:#f92672>&amp;</span>adapter<span style=color:#f92672>-&gt;</span>flags)))
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>return</span> IRQ_HANDLED;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>unlikely</span>(icr <span style=color:#f92672>&amp;</span> (E1000_ICR_RXSEQ <span style=color:#f92672>|</span> E1000_ICR_LSC))) {
</span></span><span style=display:flex><span>		hw<span style=color:#f92672>-&gt;</span>get_link_status <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>;
</span></span><span style=display:flex><span>		<span style=color:#75715e>/* guard against interrupt when we&#39;re going down */</span>
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>test_bit</span>(__E1000_DOWN, <span style=color:#f92672>&amp;</span>adapter<span style=color:#f92672>-&gt;</span>flags))
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>schedule_delayed_work</span>(<span style=color:#f92672>&amp;</span>adapter<span style=color:#f92672>-&gt;</span>watchdog_task, <span style=color:#ae81ff>1</span>);
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>/* disable interrupts, without the synchronize_irq bit */</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>ew32</span>(IMC, <span style=color:#f92672>~</span><span style=color:#ae81ff>0</span>);	<span style=color:#75715e>// 禁止IMC（Interrupt Mask Clear）中断
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#a6e22e>E1000_WRITE_FLUSH</span>();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>likely</span>(<span style=color:#a6e22e>napi_schedule_prep</span>(<span style=color:#f92672>&amp;</span>adapter<span style=color:#f92672>-&gt;</span>napi))) {
</span></span><span style=display:flex><span>		adapter<span style=color:#f92672>-&gt;</span>total_tx_bytes <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>		adapter<span style=color:#f92672>-&gt;</span>total_tx_packets <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>		adapter<span style=color:#f92672>-&gt;</span>total_rx_bytes <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>		adapter<span style=color:#f92672>-&gt;</span>total_rx_packets <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>__napi_schedule</span>(<span style=color:#f92672>&amp;</span>adapter<span style=color:#f92672>-&gt;</span>napi);	<span style=color:#75715e>// 调度napi
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	} <span style=color:#66d9ef>else</span> {
</span></span><span style=display:flex><span>		<span style=color:#75715e>/* this really should not happen! if it does it is basically a
</span></span></span><span style=display:flex><span><span style=color:#75715e>		 * bug, but not a hard error, so enable ints and continue
</span></span></span><span style=display:flex><span><span style=color:#75715e>		 */</span>
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>test_bit</span>(__E1000_DOWN, <span style=color:#f92672>&amp;</span>adapter<span style=color:#f92672>-&gt;</span>flags))
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>e1000_irq_enable</span>(adapter);
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> IRQ_HANDLED;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><code>e1000_intr</code>做的事情是不是很少？这就对了，因为Linux内核处理硬件中断使用了<strong>上半部/下半部</strong>的两阶段机制，核心目的是既要尽快响应硬件，又要避免长时间占用CPU。上半部只做<em>紧急</em>的工作，如确认中断来源、通知硬件已收到通知、唤醒下半部等。<code>e1000_intr</code>就属于中断的上半部。</p><hr><h3 id=软中断与-napi-轮询机制>软中断与 NAPI 轮询机制<a hidden class=anchor aria-hidden=true href=#软中断与-napi-轮询机制>#</a></h3><p>在上一节中，我们看到网卡触发硬中断后，e1000 驱动的中断处理函数（<code>e1000_intr</code>）主要完成了 <strong>确认事件 → 临时屏蔽中断 → 调度 NAPI</strong>。那么接下来，真正的收包工作是如何展开的呢？这就涉及到 Linux 网络子系统里的 <strong>软中断</strong> 和 <strong>NAPI</strong>。</p><h4 id=硬中断到软中断的过渡>硬中断到软中断的过渡<a hidden class=anchor aria-hidden=true href=#硬中断到软中断的过渡>#</a></h4><p><code>e1000_intr</code>调用了<code>__napi_schedule</code>来调度NAPI。<code>napi_schedule</code>还会调用<code>____napi_schedule</code>。<code>__napi_schedule()</code>的操作就是：</p><ul><li>把当前网卡对应的 <code>napi_struct</code> 对象加入到全局的 NAPI 队列；</li><li>设置 <code>NET_RX_SOFTIRQ</code> 标志，告诉内核后续需要执行网络接收的软中断。</li></ul><p>这样，真正的数据包处理就被延迟到了软中断上下文执行，避免在硬中断上下文里做大量工作。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#75715e>// file: net/core/dev.c
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>/* Called with irq disabled */</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>inline</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>____napi_schedule</span>(<span style=color:#66d9ef>struct</span> softnet_data <span style=color:#f92672>*</span>sd,
</span></span><span style=display:flex><span>				     <span style=color:#66d9ef>struct</span> napi_struct <span style=color:#f92672>*</span>napi)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>list_add_tail</span>(<span style=color:#f92672>&amp;</span>napi<span style=color:#f92672>-&gt;</span>poll_list, <span style=color:#f92672>&amp;</span>sd<span style=color:#f92672>-&gt;</span>poll_list);	<span style=color:#75715e>// 把napi_struct对象加入到全局的NAPI队列
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#a6e22e>__raise_softirq_irqoff</span>(NET_RX_SOFTIRQ);				<span style=color:#75715e>// 设置NET_RX_SOFTIRQ标志，告诉内核后续要执行网络接收的软中断
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>void</span> <span style=color:#a6e22e>__napi_schedule</span>(<span style=color:#66d9ef>struct</span> napi_struct <span style=color:#f92672>*</span>n)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>long</span> flags;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>local_irq_save</span>(flags);
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>____napi_schedule</span>(<span style=color:#a6e22e>this_cpu_ptr</span>(<span style=color:#f92672>&amp;</span>softnet_data), n); 	<span style=color:#75715e>// 调用____napi_schedule
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#a6e22e>local_irq_restore</span>(flags);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span><span style=color:#a6e22e>EXPORT_SYMBOL</span>(__napi_schedule);
</span></span></code></pre></div><p>软中断由 <code>do_softirq()</code> 执行，其中会调用 <code>net_rx_action()</code>，这是因为net_dev在初始化的时候会注册<code>NET_RX_SOFTIRQ</code>和<code>NET_TX_SOFTIRQ</code>软中断。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#75715e>// file: net/core/dev.c
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>/*
</span></span></span><span style=display:flex><span><span style=color:#75715e> *	Initialize the DEV module. At boot time this walks the device list and
</span></span></span><span style=display:flex><span><span style=color:#75715e> *	unhooks any devices that fail to initialise (normally hardware not
</span></span></span><span style=display:flex><span><span style=color:#75715e> *	present) and leaves us with a valid list of present and active devices.
</span></span></span><span style=display:flex><span><span style=color:#75715e> *
</span></span></span><span style=display:flex><span><span style=color:#75715e> */</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>/*
</span></span></span><span style=display:flex><span><span style=color:#75715e> *       This is called single threaded during boot, so no need
</span></span></span><span style=display:flex><span><span style=color:#75715e> *       to take the rtnl semaphore.
</span></span></span><span style=display:flex><span><span style=color:#75715e> */</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>int</span> __init <span style=color:#a6e22e>net_dev_init</span>(<span style=color:#66d9ef>void</span>)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>int</span> i, rc <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>ENOMEM;
</span></span><span style=display:flex><span>	
</span></span><span style=display:flex><span>    <span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>open_softirq</span>(NET_TX_SOFTIRQ, net_tx_action);	<span style=color:#75715e>// 指定由 net_tx_action 处理 NET_TX_SOFTIRQ 软中断
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#a6e22e>open_softirq</span>(NET_RX_SOFTIRQ, net_rx_action);	<span style=color:#75715e>// 指定由 net_rx_action 处理 NET_RX_SOFTIRQ 软中断
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#66d9ef>return</span> rc;
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>subsys_initcall</span>(net_dev_init);
</span></span></code></pre></div><p><code>net_rx_action()</code> 的职责是轮询 NAPI 队列，逐个调用设备驱动注册的 <code>poll</code> 方法。 对 e1000 驱动来说，这个 poll 函数就是 <code>e1000_clean()</code>。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#75715e>// file: net/core/dev.c
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>net_rx_action</span>(<span style=color:#66d9ef>struct</span> softirq_action <span style=color:#f92672>*</span>h)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>	<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> (;;) {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>struct</span> napi_struct <span style=color:#f92672>*</span>n;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>		n <span style=color:#f92672>=</span> <span style=color:#a6e22e>list_first_entry</span>(<span style=color:#f92672>&amp;</span>list, <span style=color:#66d9ef>struct</span> napi_struct, poll_list);	<span style=color:#75715e>// 从napi struct队列中取出napi对象
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		budget <span style=color:#f92672>-=</span> <span style=color:#a6e22e>napi_poll</span>(n, <span style=color:#f92672>&amp;</span>repoll);	<span style=color:#75715e>// 将对象交给 napi_poll 处理，对于e1000驱动来说，poll就是e1000_clean
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>		<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>int</span> <span style=color:#a6e22e>napi_poll</span>(<span style=color:#66d9ef>struct</span> napi_struct <span style=color:#f92672>*</span>n, <span style=color:#66d9ef>struct</span> list_head <span style=color:#f92672>*</span>repoll)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>void</span> <span style=color:#f92672>*</span>have;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>int</span> work, weight;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	work <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>test_bit</span>(NAPI_STATE_SCHED, <span style=color:#f92672>&amp;</span>n<span style=color:#f92672>-&gt;</span>state)) {
</span></span><span style=display:flex><span>		work <span style=color:#f92672>=</span> n<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>poll</span>(n, weight);	<span style=color:#75715e>// 调用 napi struct 对象的 poll 函数处理
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		<span style=color:#a6e22e>trace_napi_poll</span>(n);
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> work;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>最终软中断还是调用了e1000网卡的驱动函数<code>e1000_clean</code>。e1000_clean的职责是：在<em>软中断上下文中</em>处理收包与发包的完成逻辑。这里我们只关注收包，忽略发包。发包的逻辑实际是调用<code>adapter->clean_rx</code>来完成的。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#75715e>// file: drivers/net/ethernet/intel/e1000/e1000_main.c
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>/**
</span></span></span><span style=display:flex><span><span style=color:#75715e> * e1000_clean - NAPI Rx polling callback
</span></span></span><span style=display:flex><span><span style=color:#75715e> * @adapter: board private structure
</span></span></span><span style=display:flex><span><span style=color:#75715e> **/</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>int</span> <span style=color:#a6e22e>e1000_clean</span>(<span style=color:#66d9ef>struct</span> napi_struct <span style=color:#f92672>*</span>napi, <span style=color:#66d9ef>int</span> budget)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>struct</span> e1000_adapter <span style=color:#f92672>*</span>adapter <span style=color:#f92672>=</span> <span style=color:#a6e22e>container_of</span>(napi, <span style=color:#66d9ef>struct</span> e1000_adapter,
</span></span><span style=display:flex><span>						     napi);
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>int</span> tx_clean_complete <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>, work_done <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>    <span style=color:#75715e>/* 1. 回收发送队列 */</span>
</span></span><span style=display:flex><span>	tx_clean_complete <span style=color:#f92672>=</span> <span style=color:#a6e22e>e1000_clean_tx_irq</span>(adapter, <span style=color:#f92672>&amp;</span>adapter<span style=color:#f92672>-&gt;</span>tx_ring[<span style=color:#ae81ff>0</span>]);
</span></span><span style=display:flex><span>    <span style=color:#75715e>/* 2. 处理接收队列 */</span>
</span></span><span style=display:flex><span>	adapter<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>clean_rx</span>(adapter, <span style=color:#f92672>&amp;</span>adapter<span style=color:#f92672>-&gt;</span>rx_ring[<span style=color:#ae81ff>0</span>], <span style=color:#f92672>&amp;</span>work_done, budget);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>/* 3. 如果发送队列没清理干净，强制认为收包已经达到 budget */</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>tx_clean_complete)
</span></span><span style=display:flex><span>		work_done <span style=color:#f92672>=</span> budget;
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e>/* 4. 如果收发工作没用完 budget，说明任务完成，可以退出 NAPI 轮询 */</span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>/* If budget not fully consumed, exit the polling mode */</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>if</span> (work_done <span style=color:#f92672>&lt;</span> budget) {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>likely</span>(adapter<span style=color:#f92672>-&gt;</span>itr_setting <span style=color:#f92672>&amp;</span> <span style=color:#ae81ff>3</span>))
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>e1000_set_itr</span>(adapter);				<span style=color:#75715e>// 动态调整中断频率
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		<span style=color:#a6e22e>napi_complete_done</span>(napi, work_done);	<span style=color:#75715e>// 标记 NAPI 处理完成
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>test_bit</span>(__E1000_DOWN, <span style=color:#f92672>&amp;</span>adapter<span style=color:#f92672>-&gt;</span>flags))
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>e1000_irq_enable</span>(adapter);			<span style=color:#75715e>// 重新开启中断
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> work_done;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>为什么发包直接调用<code>e1000_clean_tx_irq</code>，而收报不是直接调用<code>e1000_clean_rx_irq</code>呢？这是因为处理收包的函数有多个，具体使用哪个是由配置决定的。具体来说就是，如果MTU大于标准以太网数据长度<code>ETH_DATA_LEN</code>（1500 字节）就使用专门处理巨帧的函数，否则就使用普通的收包函数。后面我们以普通收包函数（<code>e1000_clean_rx_irq</code>）为例进行分析。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#75715e>// file: drivers/net/ethernet/intel/e1000/e1000_main.c
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>e1000_configure_rx</span>(<span style=color:#66d9ef>struct</span> e1000_adapter <span style=color:#f92672>*</span>adapter)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>	<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>if</span> (adapter<span style=color:#f92672>-&gt;</span>netdev<span style=color:#f92672>-&gt;</span>mtu <span style=color:#f92672>&gt;</span> ETH_DATA_LEN) { 			<span style=color:#75715e>// 检查 mtu 是否大于 ETH_DATA_LEN(1500)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		rdlen <span style=color:#f92672>=</span> adapter<span style=color:#f92672>-&gt;</span>rx_ring[<span style=color:#ae81ff>0</span>].count <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span>		        <span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>struct</span> e1000_rx_desc);
</span></span><span style=display:flex><span>		adapter<span style=color:#f92672>-&gt;</span>clean_rx <span style=color:#f92672>=</span> e1000_clean_jumbo_rx_irq;	<span style=color:#75715e>// 使用专门处理jumbo frame的收包函数
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		adapter<span style=color:#f92672>-&gt;</span>alloc_rx_buf <span style=color:#f92672>=</span> e1000_alloc_jumbo_rx_buffers;
</span></span><span style=display:flex><span>	} <span style=color:#66d9ef>else</span> {
</span></span><span style=display:flex><span>		rdlen <span style=color:#f92672>=</span> adapter<span style=color:#f92672>-&gt;</span>rx_ring[<span style=color:#ae81ff>0</span>].count <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span>		        <span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>struct</span> e1000_rx_desc);
</span></span><span style=display:flex><span>		adapter<span style=color:#f92672>-&gt;</span>clean_rx <span style=color:#f92672>=</span> e1000_clean_rx_irq;			<span style=color:#75715e>// 使用普通收包函数
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		adapter<span style=color:#f92672>-&gt;</span>alloc_rx_buf <span style=color:#f92672>=</span> e1000_alloc_rx_buffers;
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>}
</span></span></code></pre></div><p>概括来说<code>e1000_clean_rx_irq</code>从DMA ring取出报文，封装成skb，然后交给<code>e1000_receive_skb</code>处理。具体包括：</p><p><strong>检查 DMA 完成标志</strong></p><ul><li>每个 <code>rx_desc</code>（接收描述符）在网卡写完数据后，会设置 <code>E1000_RXD_STAT_DD</code>。</li><li>驱动只处理已经 DMA 完成的 buffer。</li></ul><p><strong>构造 skb</strong></p><ul><li><strong>小包</strong>：调用 <code>e1000_copybreak()</code>，复制数据到新建的 <code>skb</code>。</li><li><strong>大包</strong>：直接用 <code>build_skb()</code>，避免复制开销，直接复用 DMA buffer。</li></ul><p><strong>丢弃非法数据包</strong></p><ul><li>如果包跨越多个 buffer（<code>EOP</code> 未置位），直接丢弃。</li><li>硬件发现的错误帧（CRC 错误等）也会丢弃。</li></ul><p><strong>合法数据包处理</strong></p><ul><li>调整 <code>skb</code> 长度，去掉以太网 CRC。</li><li>调用 <code>e1000_rx_checksum()</code> 做硬件校验和 offload。</li><li>调用 <code>e1000_receive_skb()</code> 把报文交给 Linux 网络协议栈（从这一刻开始，数据包就进入了 TCP/IP 栈）。</li></ul><p><strong>回收和补充 buffer</strong></p><ul><li>清空 <code>rx_desc->status</code>，表示这个描述符可以再次被网卡使用。</li><li>定期调用 <code>alloc_rx_buf()</code> 给网卡补充新的空 buffer，避免 ring buffer 被耗尽。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#75715e>// file: drivers/net/ethernet/intel/e1000/e1000_main.c
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>/**
</span></span></span><span style=display:flex><span><span style=color:#75715e> * e1000_clean_rx_irq - Send received data up the network stack; legacy
</span></span></span><span style=display:flex><span><span style=color:#75715e> * @adapter: board private structure
</span></span></span><span style=display:flex><span><span style=color:#75715e> * @rx_ring: ring to clean
</span></span></span><span style=display:flex><span><span style=color:#75715e> * @work_done: amount of napi work completed this call
</span></span></span><span style=display:flex><span><span style=color:#75715e> * @work_to_do: max amount of work allowed for this call to do
</span></span></span><span style=display:flex><span><span style=color:#75715e> */</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>bool</span> <span style=color:#a6e22e>e1000_clean_rx_irq</span>(<span style=color:#66d9ef>struct</span> e1000_adapter <span style=color:#f92672>*</span>adapter,
</span></span><span style=display:flex><span>			       <span style=color:#66d9ef>struct</span> e1000_rx_ring <span style=color:#f92672>*</span>rx_ring,
</span></span><span style=display:flex><span>			       <span style=color:#66d9ef>int</span> <span style=color:#f92672>*</span>work_done, <span style=color:#66d9ef>int</span> work_to_do)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>struct</span> net_device <span style=color:#f92672>*</span>netdev <span style=color:#f92672>=</span> adapter<span style=color:#f92672>-&gt;</span>netdev;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>struct</span> pci_dev <span style=color:#f92672>*</span>pdev <span style=color:#f92672>=</span> adapter<span style=color:#f92672>-&gt;</span>pdev;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>struct</span> e1000_rx_desc <span style=color:#f92672>*</span>rx_desc, <span style=color:#f92672>*</span>next_rxd;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>struct</span> e1000_rx_buffer <span style=color:#f92672>*</span>buffer_info, <span style=color:#f92672>*</span>next_buffer;
</span></span><span style=display:flex><span>	u32 length;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>int</span> i;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>int</span> cleaned_count <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>bool</span> cleaned <span style=color:#f92672>=</span> false;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>int</span> total_rx_bytes<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, total_rx_packets<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	i <span style=color:#f92672>=</span> rx_ring<span style=color:#f92672>-&gt;</span>next_to_clean;
</span></span><span style=display:flex><span>	rx_desc <span style=color:#f92672>=</span> <span style=color:#a6e22e>E1000_RX_DESC</span>(<span style=color:#f92672>*</span>rx_ring, i);
</span></span><span style=display:flex><span>	buffer_info <span style=color:#f92672>=</span> <span style=color:#f92672>&amp;</span>rx_ring<span style=color:#f92672>-&gt;</span>buffer_info[i];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>while</span> (rx_desc<span style=color:#f92672>-&gt;</span>status <span style=color:#f92672>&amp;</span> E1000_RXD_STAT_DD) {	<span style=color:#75715e>// 检查DMA是否完成，DD = Descriptor Done
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		<span style=color:#66d9ef>struct</span> sk_buff <span style=color:#f92672>*</span>skb;
</span></span><span style=display:flex><span>		u8 <span style=color:#f92672>*</span>data;
</span></span><span style=display:flex><span>		u8 status;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> (<span style=color:#f92672>*</span>work_done <span style=color:#f92672>&gt;=</span> work_to_do)
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>break</span>;
</span></span><span style=display:flex><span>		(<span style=color:#f92672>*</span>work_done)<span style=color:#f92672>++</span>;
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>dma_rmb</span>(); <span style=color:#75715e>/* read descriptor and rx_buffer_info after status DD */</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		status <span style=color:#f92672>=</span> rx_desc<span style=color:#f92672>-&gt;</span>status;
</span></span><span style=display:flex><span>		length <span style=color:#f92672>=</span> <span style=color:#a6e22e>le16_to_cpu</span>(rx_desc<span style=color:#f92672>-&gt;</span>length);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		data <span style=color:#f92672>=</span> buffer_info<span style=color:#f92672>-&gt;</span>rxbuf.data;		<span style=color:#75715e>// data就是真正的buffer
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		<span style=color:#a6e22e>prefetch</span>(data);
</span></span><span style=display:flex><span>        <span style=color:#75715e>// 构造skb，如果是小包，直接把数据包内从从buffer中copy一份，否则直接使用buffer
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		skb <span style=color:#f92672>=</span> <span style=color:#a6e22e>e1000_copybreak</span>(adapter, buffer_info, length, data); <span style=color:#75715e>// 为小包分配skb，如果是大包，返回NULL
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>skb) {	<span style=color:#75715e>// 处理大包
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>			<span style=color:#66d9ef>unsigned</span> <span style=color:#66d9ef>int</span> frag_len <span style=color:#f92672>=</span> <span style=color:#a6e22e>e1000_frag_len</span>(adapter);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>			skb <span style=color:#f92672>=</span> <span style=color:#a6e22e>build_skb</span>(data <span style=color:#f92672>-</span> E1000_HEADROOM, frag_len); <span style=color:#75715e>// 为大包构造skb
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>			<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>process_skb:
</span></span><span style=display:flex><span>		total_rx_bytes <span style=color:#f92672>+=</span> (length <span style=color:#f92672>-</span> <span style=color:#ae81ff>4</span>); <span style=color:#75715e>/* don&#39;t count FCS */</span>
</span></span><span style=display:flex><span>		total_rx_packets<span style=color:#f92672>++</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>likely</span>(<span style=color:#f92672>!</span>(netdev<span style=color:#f92672>-&gt;</span>features <span style=color:#f92672>&amp;</span> NETIF_F_RXFCS)))
</span></span><span style=display:flex><span>			<span style=color:#75715e>/* adjust length to remove Ethernet CRC, this must be
</span></span></span><span style=display:flex><span><span style=color:#75715e>			 * done after the TBI_ACCEPT workaround above
</span></span></span><span style=display:flex><span><span style=color:#75715e>			 */</span>
</span></span><span style=display:flex><span>			length <span style=color:#f92672>-=</span> <span style=color:#ae81ff>4</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>// 调整skb
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		<span style=color:#66d9ef>if</span> (buffer_info<span style=color:#f92672>-&gt;</span>rxbuf.data <span style=color:#f92672>==</span> NULL)
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>skb_put</span>(skb, length);
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>else</span> <span style=color:#75715e>/* copybreak skb */</span>
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>skb_trim</span>(skb, length);
</span></span><span style=display:flex><span>		<span style=color:#75715e>// 
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		<span style=color:#75715e>/* Receive Checksum Offload */</span>
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>e1000_rx_checksum</span>(adapter,
</span></span><span style=display:flex><span>				  (u32)(status) <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span>				  ((u32)(rx_desc<span style=color:#f92672>-&gt;</span>errors) <span style=color:#f92672>&lt;&lt;</span> <span style=color:#ae81ff>24</span>),
</span></span><span style=display:flex><span>				  <span style=color:#a6e22e>le16_to_cpu</span>(rx_desc<span style=color:#f92672>-&gt;</span>csum), skb);
</span></span><span style=display:flex><span>		<span style=color:#75715e>// 交给e1000_receive_skb处理
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		<span style=color:#a6e22e>e1000_receive_skb</span>(adapter, status, rx_desc<span style=color:#f92672>-&gt;</span>special, skb);
</span></span><span style=display:flex><span>		<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	}
</span></span><span style=display:flex><span>	<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 更新统计信息
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	adapter<span style=color:#f92672>-&gt;</span>total_rx_packets <span style=color:#f92672>+=</span> total_rx_packets;
</span></span><span style=display:flex><span>	adapter<span style=color:#f92672>-&gt;</span>total_rx_bytes <span style=color:#f92672>+=</span> total_rx_bytes;
</span></span><span style=display:flex><span>	netdev<span style=color:#f92672>-&gt;</span>stats.rx_bytes <span style=color:#f92672>+=</span> total_rx_bytes;
</span></span><span style=display:flex><span>	netdev<span style=color:#f92672>-&gt;</span>stats.rx_packets <span style=color:#f92672>+=</span> total_rx_packets;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> cleaned;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><code>e1000_receive_skb</code>根据协议（对于e1000网卡来说，就是以太网协议）调整skb的相关字段（如mac_header、data等），然后将skb交给<code>napi_gro_receive</code>处理。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#75715e>// file: drivers/net/ethernet/intel/e1000/e1000_main.c
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>/**
</span></span></span><span style=display:flex><span><span style=color:#75715e> * e1000_receive_skb - helper function to handle rx indications
</span></span></span><span style=display:flex><span><span style=color:#75715e> * @adapter: board private structure
</span></span></span><span style=display:flex><span><span style=color:#75715e> * @status: descriptor status field as written by hardware
</span></span></span><span style=display:flex><span><span style=color:#75715e> * @vlan: descriptor vlan field as written by hardware (no le/be conversion)
</span></span></span><span style=display:flex><span><span style=color:#75715e> * @skb: pointer to sk_buff to be indicated to stack
</span></span></span><span style=display:flex><span><span style=color:#75715e> */</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>e1000_receive_skb</span>(<span style=color:#66d9ef>struct</span> e1000_adapter <span style=color:#f92672>*</span>adapter, u8 status,
</span></span><span style=display:flex><span>			      __le16 vlan, <span style=color:#66d9ef>struct</span> sk_buff <span style=color:#f92672>*</span>skb)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>	skb<span style=color:#f92672>-&gt;</span>protocol <span style=color:#f92672>=</span> <span style=color:#a6e22e>eth_type_trans</span>(skb, adapter<span style=color:#f92672>-&gt;</span>netdev);	<span style=color:#75715e>// 调整skb的mac_header，根据L2 header判断协议类型，并将skb-&gt;data指向下一层协议头
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>if</span> (status <span style=color:#f92672>&amp;</span> E1000_RXD_STAT_VP) {
</span></span><span style=display:flex><span>		u16 vid <span style=color:#f92672>=</span> <span style=color:#a6e22e>le16_to_cpu</span>(vlan) <span style=color:#f92672>&amp;</span> E1000_RXD_SPC_VLAN_MASK;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>__vlan_hwaccel_put_tag</span>(skb, <span style=color:#a6e22e>htons</span>(ETH_P_8021Q), vid);
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>napi_gro_receive</span>(<span style=color:#f92672>&amp;</span>adapter<span style=color:#f92672>-&gt;</span>napi, skb);	<span style=color:#75715e>// 交给上层协议栈处理
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>}
</span></span></code></pre></div><p><code>napi_gro_receive</code>是Linux内核<strong>GRO</strong>（Generic Receive Offload）接收路径的一个函数，实现了对接收到的skb进行聚合处理。这里我们不关心具体的聚合操作细节（只需要知道<code>dev_gro_receive</code>会给<code>napi_skb_finish</code>返回聚合的结果即可），直接看<code>napi_skb_finish</code>是如何处理skb的。<code>napi_skb_finish</code>根据聚合结果判断是否继续处理skb，正常情况下会将skb交给其他函数处理，最终skb会被<code>__netif_receive_skb_core</code>处理。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#75715e>// file: net/core/dev.c
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>gro_result_t</span> <span style=color:#a6e22e>napi_gro_receive</span>(<span style=color:#66d9ef>struct</span> napi_struct <span style=color:#f92672>*</span>napi, <span style=color:#66d9ef>struct</span> sk_buff <span style=color:#f92672>*</span>skb)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>trace_napi_gro_receive_entry</span>(skb);	<span style=color:#75715e>// 追踪调试入口，可用于ftrace等跟踪工具
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>skb_gro_reset_offset</span>(skb);			<span style=color:#75715e>// 重置 skb 的 GRO 状态，让 skb 可以参与聚合
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 调用 dev_gro_receive 将 skb 交给设备层的 GRO，
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// dev_gro_receive 会尝试将多个小包聚合成大包，提高处理效率
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// napi_skb_finish 根据 dev_gro_receive 的返回结果做后续处理
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>napi_skb_finish</span>(<span style=color:#a6e22e>dev_gro_receive</span>(napi, skb), skb);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span><span style=color:#a6e22e>EXPORT_SYMBOL</span>(napi_gro_receive);
</span></span></code></pre></div><p><code>__netif_receive_skb_core</code>比较长，以下是保留了skb分发逻辑的简化版本。主要流程如下：</p><ol><li><p>先分发给全局链表 <code>ptype_all</code>（tcpdump 等监听工具会在这里收到数据包）。</p></li><li><p>再分发给网卡专属链表 <code>dev->ptype_all</code>（网卡功能相关）。</p></li><li><p>调用网卡 <code>rx_handler</code>（驱动自定义的接收回调）。</p></li><li><p>根据协议类型分发给 IPv4/IPv6/ARP等处理。</p></li><li><p>最后调用最后一个 packet_type 的处理函数或丢弃。</p></li></ol><p>对于IP数据包，就会分发给<code>ip_rcv</code>函数处理。至此网络数据包就从网卡到达到了网络协议栈的入口。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#75715e>// file: net/core/dev.c
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>int</span> <span style=color:#a6e22e>__netif_receive_skb_core</span>(<span style=color:#66d9ef>struct</span> sk_buff <span style=color:#f92672>*</span>skb)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>struct</span> packet_type <span style=color:#f92672>*</span>ptype, <span style=color:#f92672>*</span>pt_prev;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>rx_handler_func_t</span> <span style=color:#f92672>*</span>rx_handler;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>struct</span> net_device <span style=color:#f92672>*</span>orig_dev;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> ret <span style=color:#f92672>=</span> NET_RX_DROP;
</span></span><span style=display:flex><span>    __be16 type;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    orig_dev <span style=color:#f92672>=</span> skb<span style=color:#f92672>-&gt;</span>dev;
</span></span><span style=display:flex><span>    pt_prev <span style=color:#f92672>=</span> NULL;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 1. 分发给全局 packet_type 链表（所有网卡共享）
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#a6e22e>list_for_each_entry_rcu</span>(ptype, <span style=color:#f92672>&amp;</span>ptype_all, list) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (pt_prev)
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>deliver_skb</span>(skb, pt_prev, orig_dev);
</span></span><span style=display:flex><span>        pt_prev <span style=color:#f92672>=</span> ptype;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 2. 分发给当前网卡专属 packet_type 链表
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#a6e22e>list_for_each_entry_rcu</span>(ptype, <span style=color:#f92672>&amp;</span>skb<span style=color:#f92672>-&gt;</span>dev<span style=color:#f92672>-&gt;</span>ptype_all, list) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (pt_prev)
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>deliver_skb</span>(skb, pt_prev, orig_dev);
</span></span><span style=display:flex><span>        pt_prev <span style=color:#f92672>=</span> ptype;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 3. 调用网卡驱动可能设置的 rx_handler
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    rx_handler <span style=color:#f92672>=</span> <span style=color:#a6e22e>rcu_dereference</span>(skb<span style=color:#f92672>-&gt;</span>dev<span style=color:#f92672>-&gt;</span>rx_handler);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (rx_handler) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (pt_prev) {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>deliver_skb</span>(skb, pt_prev, orig_dev);
</span></span><span style=display:flex><span>            pt_prev <span style=color:#f92672>=</span> NULL;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>switch</span> (<span style=color:#a6e22e>rx_handler</span>(<span style=color:#f92672>&amp;</span>skb)) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>case</span> RX_HANDLER_CONSUMED:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> NET_RX_SUCCESS;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>case</span> RX_HANDLER_ANOTHER:
</span></span><span style=display:flex><span>            <span style=color:#75715e>// skb 被重定向，需要再次分发
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>            pt_prev <span style=color:#f92672>=</span> NULL;
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>case</span> RX_HANDLER_EXACT:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>case</span> RX_HANDLER_PASS:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 4. 按协议类型分发给对应链表（IPv4/IPv6/ARP）
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    type <span style=color:#f92672>=</span> skb<span style=color:#f92672>-&gt;</span>protocol;
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>deliver_ptype_list_skb</span>(skb, <span style=color:#f92672>&amp;</span>pt_prev, orig_dev, type,
</span></span><span style=display:flex><span>                           <span style=color:#f92672>&amp;</span>ptype_base[<span style=color:#a6e22e>ntohs</span>(type) <span style=color:#f92672>&amp;</span> PTYPE_HASH_MASK]);	<span style=color:#75715e>// 对于TCP/UDP，就是在这里分发给IPv4处理的
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#a6e22e>deliver_ptype_list_skb</span>(skb, <span style=color:#f92672>&amp;</span>pt_prev, orig_dev, type,
</span></span><span style=display:flex><span>                           <span style=color:#f92672>&amp;</span>orig_dev<span style=color:#f92672>-&gt;</span>ptype_specific);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 5. 最终调用最后一个 packet_type 的处理函数，或者丢弃
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>if</span> (pt_prev)
</span></span><span style=display:flex><span>        ret <span style=color:#f92672>=</span> pt_prev<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>func</span>(skb, skb<span style=color:#f92672>-&gt;</span>dev, pt_prev, orig_dev);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span> {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>kfree_skb</span>(skb);
</span></span><span style=display:flex><span>        ret <span style=color:#f92672>=</span> NET_RX_DROP;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ret;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>这里插播一段内容：tcpdump的抓包的实现原理其实就是创建<code>PF_PACKET</code>类型的socket并添加到<code>ptype_all</code>或者<code>dev->ptype_all</code>，而每一个数据包都会经历前面的1、2两个分发过程。可以看到tcpdump的抓包位置在数据包处理的早期（入向流量），所以即使数据包会被丢弃，tcpdump也能抓到完整的数据包。</p><p>如何确定把数据包分发给谁处理呢？对于IPv4数据包而言，e1000网卡在将skb交给<code>napi_gro_receive</code>前，会设置<code>skb->protocol</code>，<code>__netif_receive_skb_core</code>在分发的时候会在ptype_base中查找匹配的处理函数，对于IPv4而言，在初始化的时候会将<code>ip_packet_type</code>添加到<code>ptype_base</code>中。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#75715e>// file: net/ipv4/af_inet.c
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>struct</span> packet_type ip_packet_type __read_mostly <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>	.type <span style=color:#f92672>=</span> <span style=color:#a6e22e>cpu_to_be16</span>(ETH_P_IP),
</span></span><span style=display:flex><span>	.func <span style=color:#f92672>=</span> ip_rcv,		<span style=color:#75715e>// 处理IP数据包的函数
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>};
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>static</span> <span style=color:#66d9ef>int</span> __init <span style=color:#a6e22e>inet_init</span>(<span style=color:#66d9ef>void</span>)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>	<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#a6e22e>dev_add_pack</span>(<span style=color:#f92672>&amp;</span>ip_packet_type);
</span></span><span style=display:flex><span>	<span style=color:#75715e>// ......
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>}
</span></span></code></pre></div><h1 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h1><p>经过一通分析，终于把数据包从网卡送到了内核网络协议栈，还意外的发现了tcpdump抓包的位置（入向包），真不容易。回过头来再看这个过程：</p><ol><li>数据包达到网卡；</li><li>网卡通过DMA将数据包写入驱动预先分配好的接收缓冲区；</li><li>网卡通过IRQ告诉CPU有新的网络数据包到达；</li><li>CPU快速响应网卡的IRQ请求，简单调用<code>__napi_schedule()</code>，把NAPI poll加入调度队列；</li><li>软中断调度NAPI poll，最终调用e1000网卡的poll函数（<code>e1000_clean</code>）；</li><li>网卡的poll函读取DMA ring描述符，把数据包封装进sk_buff结构体（即skb），交给内核网络子系统；</li><li>内核网络子系统按照协议类型将skb分发给对应的处理函数，对于IP数据包而言，处理函数就是<code>ip_rcv</code>；</li></ol><p>在分析的过程中，我们还看到了在分发skb时会遍历ptype_all，如果使用tcpdump抓包，tcpdump创建的socket就会被添加到ptype_all中，所以就能抓到L2~L7的完整数据包。</p><p>ip_rcv如何处理数据包的呢？这涉及到路由以及分发给L4层处理（如TCP/UDP）。这块内容留在下一篇文章介绍。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://rand0m42195.github.io/tags/networking/>Networking</a></li><li><a href=https://rand0m42195.github.io/tags/linux/>Linux</a></li><li><a href=https://rand0m42195.github.io/tags/c/>C</a></li></ul><nav class=paginav><a class=next href=https://rand0m42195.github.io/posts/first-blog/><span class=title>下一页 »</span><br><span>First Blog</span></a></nav></footer><div id=comments><script src=https://giscus.app/client.js data-repo=rand0m42195/rand0m42195.github.io data-repo-id=R_kgDOPzDcOw data-category=Announcements data-category-id=DIC_kwDOPzDcO84CvpUm data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://rand0m42195.github.io/>rand0m's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>