<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Kernel on rand0m's blog</title><link>https://rand0m42195.github.io/tags/kernel/</link><description>Recent content in Kernel on rand0m's blog</description><generator>Hugo -- 0.150.0</generator><language>zh</language><lastBuildDate>Thu, 25 Sep 2025 11:12:40 +0800</lastBuildDate><atom:link href="https://rand0m42195.github.io/tags/kernel/index.xml" rel="self" type="application/rss+xml"/><item><title>XDP 挂载模式剖析</title><link>https://rand0m42195.github.io/posts/linux-ebpf-xdp-models/</link><pubDate>Thu, 25 Sep 2025 11:12:40 +0800</pubDate><guid>https://rand0m42195.github.io/posts/linux-ebpf-xdp-models/</guid><description>&lt;h2 id="xdp是什么"&gt;XDP是什么？&lt;/h2&gt;
&lt;p&gt;了解&lt;code&gt;XDP&lt;/code&gt;的读者应该知道：&lt;code&gt;XDP&lt;/code&gt;是基于&lt;code&gt;eBPF&lt;/code&gt;的一个高性能网络路径技术，它的原理就是在数据包处理的早期阶段（在内核网络协议栈之前）挂载&lt;code&gt;eBPF&lt;/code&gt;程序对数据包进行处理，从而实现高效的网络数据包处理。如果你写过&lt;code&gt;XDP&lt;/code&gt;程序，那么一定知道挂载&lt;code&gt;XDP&lt;/code&gt;的时候有多种模式可选，不同模式之间的效率不同。这篇文章我们就来深入剖析一下&lt;code&gt;XDP&lt;/code&gt;的集中模式之间到底有哪些区别。&lt;/p&gt;
&lt;p&gt;首先看看&lt;code&gt;XDP&lt;/code&gt;挂载模式有哪几种？不同的挂载模式有和区别？&lt;/p&gt;
&lt;p&gt;&lt;code&gt;XDP&lt;/code&gt;挂载模式可以以三种方式挂在到网卡上：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Generic&lt;/th&gt;
&lt;th&gt;Native&lt;/th&gt;
&lt;th&gt;Offloaded&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;兼容性&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;兼容所有网络设备&lt;/td&gt;
&lt;td&gt;需要网卡驱动显示支持XDP&lt;/td&gt;
&lt;td&gt;特定的可编程网卡&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;执行阶段&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;在网络核心代码中执行（此时已经分配了SKB）&lt;/td&gt;
&lt;td&gt;在网卡驱动中执行（还未分配SKB）&lt;/td&gt;
&lt;td&gt;网卡执行，CPU零开销&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;性能&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;较低&lt;/td&gt;
&lt;td&gt;高&lt;/td&gt;
&lt;td&gt;最高&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="xdp-挂载原理"&gt;XDP 挂载原理&lt;/h2&gt;
&lt;p&gt;XDP程序是挂载在网络数据包的处理路径上的，所以我们有必要先对网络数据包的处理路径有一个整体的掌握（这里插播一条小广告，我之前写过一篇分析&lt;a href="https://rand0m42195.github.io/posts/linux-networking-receive/"&gt;数据包从网卡到内核协议栈&lt;/a&gt;的博客）。&lt;/p&gt;
&lt;p&gt;数据包从网卡到内核网络协议栈的流程可以分为以下几个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;数据包到达网卡&lt;/strong&gt; 网卡硬件接收以太帧，做基本校验（如 CRC）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DMA 写入内存&lt;/strong&gt; 网卡通过 DMA 将数据包写入驱动预先分配好的接收缓冲区（Descriptor Ring）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;中断通知 CPU&lt;/strong&gt; 网卡通过 IRQ 告诉 CPU：“我收到了新数据包”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;驱动中断处理函数（ISR）&lt;/strong&gt; 驱动快速处理中断，通常只是调用 &lt;code&gt;__napi_schedule()&lt;/code&gt;，把 NAPI poll 加入调度队列。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;软中断调度 NAPI poll&lt;/strong&gt; CPU 执行 &lt;code&gt;do_softirq()&lt;/code&gt; → &lt;code&gt;net_rx_action()&lt;/code&gt; → 调用 网卡的的 poll 函数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;poll 函数提取数据包并构造 skb&lt;/strong&gt; 驱动在 poll 中读取 DMA ring 的描述符，把数据包封装进 &lt;code&gt;sk_buff&lt;/code&gt; 结构，交给网络核心层。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;网络核心层处理&lt;/strong&gt; 网络核心层根据数据包格式选择对应的协议栈，然后交给协议栈处理。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;XDP就是挂载在上面的某个阶段，从而实现高效网络数据包处理的。具体来说&lt;em&gt;Native&lt;/em&gt;模式的&lt;code&gt;XDP&lt;/code&gt;是在网卡的驱动程序中执行的（对应步骤6），而Generic模式的XDP是在网络核心层中执行的（对应步骤7）。这也说明了&lt;em&gt;Native&lt;/em&gt;模式的性能比&lt;em&gt;Generic&lt;/em&gt;模式高。&lt;/p&gt;</description></item><item><title>网络数据包接受过程分析——从网卡到内核协议栈（以Intel e1000 + Linux 4.4为例）</title><link>https://rand0m42195.github.io/posts/linux-networking-receive/</link><pubDate>Mon, 22 Sep 2025 20:43:51 +0800</pubDate><guid>https://rand0m42195.github.io/posts/linux-networking-receive/</guid><description>&lt;h1 id="引言"&gt;引言&lt;/h1&gt;
&lt;p&gt;网络数据包从网卡到应用程序，需要经历一段复杂的旅程。作为开发者，我们平时调用 &lt;code&gt;socket()&lt;/code&gt;、&lt;code&gt;recv()&lt;/code&gt; 就能轻松拿到数据，却很少思考内核背后究竟发生了什么。&lt;/p&gt;
&lt;p&gt;本系列文章尝试结合 &lt;strong&gt;理论流程 + 内核源码分析&lt;/strong&gt;，逐步剖析 Linux 内核中网络数据包的接收过程。这里选择 Linux 4.4 内核作为例子（代码相对稳定，资料丰富，逻辑上没有过多新特性干扰），并结合 Intel e1000 驱动来具体展示数据包是如何从网卡到达内核网络协议栈的。&lt;/p&gt;
&lt;h1 id="网络数据包接收的总体流程"&gt;网络数据包接收的总体流程&lt;/h1&gt;
&lt;p&gt;先给出一个全局视角：数据包从网卡到达内存，再到协议栈的路径，大致如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;数据包到达网卡&lt;/strong&gt;
网卡硬件接收以太帧，做基本校验（如 CRC）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DMA 写入内存&lt;/strong&gt;
网卡通过 DMA 将数据包写入驱动预先分配好的接收缓冲区（Descriptor Ring）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;中断通知 CPU&lt;/strong&gt;
网卡通过 IRQ 告诉 CPU：“我收到了新数据包”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;驱动中断处理函数（ISR）&lt;/strong&gt;
驱动快速处理中断，通常只是调用 &lt;code&gt;__napi_schedule()&lt;/code&gt;，把 NAPI poll 加入调度队列。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;软中断调度 NAPI poll&lt;/strong&gt;
CPU 执行 &lt;code&gt;do_softirq()&lt;/code&gt; → &lt;code&gt;net_rx_action()&lt;/code&gt; → 调用 e1000 的 poll 函数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;poll 函数提取数据包并构造 skb&lt;/strong&gt;
驱动在 poll 中读取 DMA ring 的描述符，把数据包封装进 &lt;code&gt;sk_buff&lt;/code&gt; 结构，交给内核网络子系统。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;协议栈处理&lt;/strong&gt;
skb 被送到 IP 层，进一步交给 TCP/UDP，最终到达 socket，供应用程序读取。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="网卡收包流程" loading="lazy" src="https://rand0m42195.github.io/images/posts/linux-networking-receive/nic-networking-stack.png"&gt;&lt;/p&gt;</description></item></channel></rss>